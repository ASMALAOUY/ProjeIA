{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a49615c7-e3fe-40a1-9271-0d86ce533d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. EXPLORATORY DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: (13608, 20)\n",
      "\n",
      "First few rows:\n",
      "        id                                              title  \\\n",
      "0   762616  The Complete SQL Bootcamp 2020: Go from Zero t...   \n",
      "1   937678  Tableau 2020 A-Z: Hands-On Tableau Training fo...   \n",
      "2  1361790             PMP Exam Prep Seminar -  PMBOK Guide 6   \n",
      "3   648826         The Complete Financial Analyst Course 2020   \n",
      "4   637930  An Entire MBA in 1 Course:Award Winning Busine...   \n",
      "\n",
      "                                                 url  is_paid  \\\n",
      "0                 /course/the-complete-sql-bootcamp/     True   \n",
      "1                                 /course/tableau10/     True   \n",
      "2                        /course/pmp-pmbok6-35-pdus/     True   \n",
      "3     /course/the-complete-financial-analyst-course/     True   \n",
      "4  /course/an-entire-mba-in-1-courseaward-winning...     True   \n",
      "\n",
      "   num_subscribers  avg_rating  avg_rating_recent   rating  num_reviews  \\\n",
      "0           295509     4.66019            4.67874  4.67874        78006   \n",
      "1           209070     4.58956            4.60015  4.60015        54581   \n",
      "2           155282     4.59491            4.59326  4.59326        52653   \n",
      "3           245860     4.54407            4.53772  4.53772        46447   \n",
      "4           374836     4.47080            4.47173  4.47173        41630   \n",
      "\n",
      "   is_wishlisted  num_published_lectures  num_published_practice_tests  \\\n",
      "0          False                      84                             0   \n",
      "1          False                      78                             0   \n",
      "2          False                     292                             2   \n",
      "3          False                     338                             0   \n",
      "4          False                      83                             0   \n",
      "\n",
      "                created        published_time  discount_price__amount  \\\n",
      "0  2016-02-14T22:57:48Z  2016-04-06T05:16:11Z                   455.0   \n",
      "1  2016-08-22T12:10:18Z  2016-08-23T16:59:49Z                   455.0   \n",
      "2  2017-09-26T16:32:48Z  2017-11-14T23:58:14Z                   455.0   \n",
      "3  2015-10-23T13:34:35Z  2016-01-21T01:38:48Z                   455.0   \n",
      "4  2015-10-12T06:39:46Z  2016-01-11T21:39:33Z                   455.0   \n",
      "\n",
      "  discount_price__currency discount_price__price_string  price_detail__amount  \\\n",
      "0                      INR                         ‚Çπ455                8640.0   \n",
      "1                      INR                         ‚Çπ455                8640.0   \n",
      "2                      INR                         ‚Çπ455                8640.0   \n",
      "3                      INR                         ‚Çπ455                8640.0   \n",
      "4                      INR                         ‚Çπ455                8640.0   \n",
      "\n",
      "  price_detail__currency price_detail__price_string  \n",
      "0                    INR                     ‚Çπ8,640  \n",
      "1                    INR                     ‚Çπ8,640  \n",
      "2                    INR                     ‚Çπ8,640  \n",
      "3                    INR                     ‚Çπ8,640  \n",
      "4                    INR                     ‚Çπ8,640  \n",
      "\n",
      "Data Types:\n",
      "id                                int64\n",
      "title                            object\n",
      "url                              object\n",
      "is_paid                            bool\n",
      "num_subscribers                   int64\n",
      "avg_rating                      float64\n",
      "avg_rating_recent               float64\n",
      "rating                          float64\n",
      "num_reviews                       int64\n",
      "is_wishlisted                      bool\n",
      "num_published_lectures            int64\n",
      "num_published_practice_tests      int64\n",
      "created                          object\n",
      "published_time                   object\n",
      "discount_price__amount          float64\n",
      "discount_price__currency         object\n",
      "discount_price__price_string     object\n",
      "price_detail__amount            float64\n",
      "price_detail__currency           object\n",
      "price_detail__price_string       object\n",
      "dtype: object\n",
      "\n",
      "Basic Statistics:\n",
      "                 id  num_subscribers    avg_rating  avg_rating_recent  \\\n",
      "count  1.360800e+04     13608.000000  13608.000000       13608.000000   \n",
      "mean   1.681721e+06      2847.010435      3.923293           3.912242   \n",
      "std    9.539271e+05      9437.865634      1.031304           1.039237   \n",
      "min    2.762000e+03         0.000000      0.000000           0.000000   \n",
      "25%    8.580862e+05        62.000000      3.800000           3.787315   \n",
      "50%    1.623421e+06       533.000000      4.194440           4.181735   \n",
      "75%    2.503720e+06      2279.500000      4.450000           4.452105   \n",
      "max    3.486006e+06    374836.000000      5.000000           5.000000   \n",
      "\n",
      "             rating   num_reviews  num_published_lectures  \\\n",
      "count  13608.000000  13608.000000            13608.000000   \n",
      "mean       3.912242    243.169827               32.224794   \n",
      "std        1.039237   1580.965895               42.766911   \n",
      "min        0.000000      0.000000                0.000000   \n",
      "25%        3.787315      7.000000               12.000000   \n",
      "50%        4.181735     24.000000               21.000000   \n",
      "75%        4.452105     87.000000               37.000000   \n",
      "max        5.000000  78006.000000              699.000000   \n",
      "\n",
      "       num_published_practice_tests  discount_price__amount  \\\n",
      "count                  13608.000000            12205.000000   \n",
      "mean                       0.110523              493.943794   \n",
      "std                        0.623501              267.827260   \n",
      "min                        0.000000              455.000000   \n",
      "25%                        0.000000              455.000000   \n",
      "50%                        0.000000              455.000000   \n",
      "75%                        0.000000              455.000000   \n",
      "max                        6.000000             3200.000000   \n",
      "\n",
      "       price_detail__amount  \n",
      "count          13111.000000  \n",
      "mean            4646.992602  \n",
      "std             3109.101019  \n",
      "min             1280.000000  \n",
      "25%             1600.000000  \n",
      "50%             3200.000000  \n",
      "75%             8640.000000  \n",
      "max            12800.000000  \n",
      "\n",
      "================================================================================\n",
      "2. MISSING VALUES ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Missing Values:\n",
      "                                                    Column  Missing_Count  \\\n",
      "discount_price__amount              discount_price__amount           1403   \n",
      "discount_price__currency          discount_price__currency           1403   \n",
      "discount_price__price_string  discount_price__price_string           1403   \n",
      "price_detail__amount                  price_detail__amount            497   \n",
      "price_detail__currency              price_detail__currency            497   \n",
      "price_detail__price_string      price_detail__price_string            497   \n",
      "\n",
      "                              Missing_Percentage  \n",
      "discount_price__amount                     10.31  \n",
      "discount_price__currency                   10.31  \n",
      "discount_price__price_string               10.31  \n",
      "price_detail__amount                        3.65  \n",
      "price_detail__currency                      3.65  \n",
      "price_detail__price_string                  3.65  \n",
      "\n",
      "================================================================================\n",
      "3. OUTLIERS DETECTION & HANDLING\n",
      "================================================================================\n",
      "\n",
      "num_subscribers: 1614 outliers detected (11.86%)\n",
      "\n",
      "avg_rating: 944 outliers detected (6.94%)\n",
      "\n",
      "avg_rating_recent: 982 outliers detected (7.22%)\n",
      "\n",
      "rating: 982 outliers detected (7.22%)\n",
      "\n",
      "num_reviews: 1978 outliers detected (14.54%)\n",
      "\n",
      "num_published_lectures: 989 outliers detected (7.27%)\n",
      "\n",
      "num_published_practice_tests: 600 outliers detected (4.41%)\n",
      "\n",
      "discount_price__amount: 795 outliers detected (5.84%)\n",
      "Removed 1453 rows for num_reviews\n",
      "Removed 832 rows for num_subscribers\n",
      "Removed 328 rows for num_published_lectures\n",
      "\n",
      "Dataset shape after outlier removal: (10995, 20)\n",
      "\n",
      "================================================================================\n",
      "4. FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "‚úì Created temporal features: course_age_days, course_year, course_month\n",
      "‚úì Created: review_rate (reviews per subscriber)\n",
      "‚úì Created: is_highly_rated, rating_category\n",
      "‚úì Created: popularity_score (normalized)\n",
      "‚úì Created: content_density (lectures per day)\n",
      "‚úì Created: wishlist_flag\n",
      "\n",
      "================================================================================\n",
      "5. FINAL DATA VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Final dataset shape: (10995, 29)\n",
      "Rows removed: 2613\n",
      "\n",
      "New features created:\n",
      "  - content_density\n",
      "  - course_age_days\n",
      "  - course_month\n",
      "  - course_year\n",
      "  - is_highly_rated\n",
      "  - popularity_score\n",
      "  - rating_category\n",
      "  - review_rate\n",
      "  - wishlist_flag\n",
      "\n",
      "Remaining missing values: 5240\n",
      "Duplicate rows: 0\n",
      "\n",
      "Final dataset info:\n",
      "id                                            int64\n",
      "title                                        object\n",
      "url                                          object\n",
      "is_paid                                        bool\n",
      "num_subscribers                               int64\n",
      "avg_rating                                  float64\n",
      "avg_rating_recent                           float64\n",
      "rating                                      float64\n",
      "num_reviews                                   int64\n",
      "is_wishlisted                                  bool\n",
      "num_published_lectures                        int64\n",
      "num_published_practice_tests                  int64\n",
      "created                         datetime64[ns, UTC]\n",
      "published_time                  datetime64[ns, UTC]\n",
      "discount_price__amount                      float64\n",
      "discount_price__currency                     object\n",
      "discount_price__price_string                 object\n",
      "price_detail__amount                        float64\n",
      "price_detail__currency                       object\n",
      "price_detail__price_string                   object\n",
      "course_age_days                               int64\n",
      "course_year                                   int32\n",
      "course_month                                  int32\n",
      "review_rate                                 float64\n",
      "is_highly_rated                               int64\n",
      "rating_category                            category\n",
      "popularity_score                            float64\n",
      "content_density                             float64\n",
      "wishlist_flag                                 int64\n",
      "dtype: object\n",
      "\n",
      "‚úì Cleaned dataset saved to: cousedata_cleaned.csv\n",
      "\n",
      "Sample of engineered features:\n",
      "      wishlist_flag  popularity_score  course_age_days  content_density  \\\n",
      "1190              0          0.210254             2535         0.019322   \n",
      "1192              0          0.252891             3303         0.008172   \n",
      "1199              0          0.364060             3012         0.021241   \n",
      "1200              0          0.702054             2523         0.028130   \n",
      "1201              0          0.389263             4383         0.013002   \n",
      "1202              0          0.166580             3369         0.028783   \n",
      "1204              0          0.245641             2805         0.005346   \n",
      "1205              0          0.573451             3921         0.009434   \n",
      "1206              0          0.570516             3560         0.017130   \n",
      "1208              0          0.254790             2688         0.004835   \n",
      "\n",
      "      course_year  course_month  review_rate rating_category  is_highly_rated  \n",
      "1190         2018            12     0.268253         Average                0  \n",
      "1192         2016            11     0.222374            Good                0  \n",
      "1199         2017             9     0.152607       Excellent                1  \n",
      "1200         2019             1     0.079400            Good                0  \n",
      "1201         2013            12     0.142287       Excellent                1  \n",
      "1202         2016             9     0.334369       Excellent                1  \n",
      "1204         2018             3     0.224719            Good                0  \n",
      "1205         2015             3     0.096299            Good                0  \n",
      "1206         2016             3     0.096794            Good                0  \n",
      "1208         2018             7     0.215301            Good                0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cousedataset.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"1. EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nDataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MISSING VALUES ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_data if len(missing_data) > 0 else \"No missing values found\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. OUTLIERS DETECTION & REMOVAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. OUTLIERS DETECTION & HANDLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "\n",
    "# Numeric columns to check for outliers\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "outliers_summary = {}\n",
    "for col in numeric_cols:\n",
    "    outliers = detect_outliers_iqr(df, col)\n",
    "    if len(outliers) > 0:\n",
    "        outliers_summary[col] = len(outliers)\n",
    "        print(f\"\\n{col}: {len(outliers)} outliers detected ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Remove extreme outliers (keep rows where all numeric values are within reasonable bounds)\n",
    "df_clean = df.copy()\n",
    "\n",
    "# For rating column: if exists, cap between 0-5\n",
    "if 'rating' in df_clean.columns:\n",
    "    df_clean['rating'] = df_clean['rating'].clip(0, 5)\n",
    "\n",
    "# For subscriber count, reviews, etc.: remove extreme outliers\n",
    "cols_to_check = ['num_reviews', 'num_subscribers', 'num_published_lectures']\n",
    "for col in cols_to_check:\n",
    "    if col in df_clean.columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR  # Use 3*IQR for removal (more conservative)\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        initial_count = len(df_clean)\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "        print(f\"Removed {initial_count - len(df_clean)} rows for {col}\")\n",
    "\n",
    "print(f\"\\nDataset shape after outlier removal: {df_clean.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert datetime columns if they exist\n",
    "date_cols = ['created', 'published_time']\n",
    "for col in date_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "\n",
    "# Extract temporal features from created date\n",
    "if 'created' in df_clean.columns:\n",
    "    # Handle timezone-aware/naive datetime conversion\n",
    "    now = pd.Timestamp.now(tz='UTC')\n",
    "    if df_clean['created'].dt.tz is None:\n",
    "        # If naive, use naive now\n",
    "        now = datetime.now()\n",
    "    df_clean['course_age_days'] = (now - df_clean['created']).dt.days\n",
    "    df_clean['course_year'] = df_clean['created'].dt.year\n",
    "    df_clean['course_month'] = df_clean['created'].dt.month\n",
    "    print(\"\\n‚úì Created temporal features: course_age_days, course_year, course_month\")\n",
    "\n",
    "# Engagement metrics\n",
    "if 'num_reviews' in df_clean.columns and 'num_subscribers' in df_clean.columns:\n",
    "    df_clean['review_rate'] = df_clean['num_reviews'] / (df_clean['num_subscribers'] + 1)\n",
    "    df_clean['review_rate'] = df_clean['review_rate'].clip(0, 1)  # Cap at 100%\n",
    "    print(\"‚úì Created: review_rate (reviews per subscriber)\")\n",
    "\n",
    "# Rating-based features\n",
    "if 'rating' in df_clean.columns:\n",
    "    df_clean['is_highly_rated'] = (df_clean['rating'] >= 4.5).astype(int)\n",
    "    df_clean['rating_category'] = pd.cut(df_clean['rating'], \n",
    "                                         bins=[0, 3, 4, 4.5, 5], \n",
    "                                         labels=['Poor', 'Average', 'Good', 'Excellent'])\n",
    "    print(\"‚úì Created: is_highly_rated, rating_category\")\n",
    "\n",
    "# Course popularity score (composite metric)\n",
    "if 'num_subscribers' in df_clean.columns:\n",
    "    scaler = MinMaxScaler()\n",
    "    df_clean['popularity_score'] = scaler.fit_transform(\n",
    "        df_clean[['num_subscribers']]\n",
    "    )\n",
    "    print(\"‚úì Created: popularity_score (normalized)\")\n",
    "\n",
    "# Price-related features\n",
    "if 'discount_price_amount' in df_clean.columns:\n",
    "    df_clean['has_discount'] = (df_clean['discount_price_amount'] > 0).astype(int)\n",
    "    print(\"‚úì Created: has_discount\")\n",
    "\n",
    "# Content features\n",
    "if 'num_published_lectures' in df_clean.columns:\n",
    "    df_clean['content_density'] = df_clean['num_published_lectures'] / (df_clean['course_age_days'] + 1)\n",
    "    print(\"‚úì Created: content_density (lectures per day)\")\n",
    "\n",
    "# Wishlist engagement\n",
    "if 'is_wishlisted' in df_clean.columns:\n",
    "    df_clean['wishlist_flag'] = df_clean['is_wishlisted'].astype(int)\n",
    "    print(\"‚úì Created: wishlist_flag\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. DATA VALIDATION & SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. FINAL DATA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_cols = set(df_clean.columns) - set(df.columns)\n",
    "for col in sorted(new_cols):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"\\nRemaining missing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df_clean.duplicated().sum()}\")\n",
    "\n",
    "# Display cleaned dataset info\n",
    "print(\"\\nFinal dataset info:\")\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SAVE CLEANED DATASET\n",
    "# ============================================================================\n",
    "output_file = 'cousedata_cleaned.csv'\n",
    "df_clean.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úì Cleaned dataset saved to: {output_file}\")\n",
    "\n",
    "# Display sample of engineered features\n",
    "print(\"\\nSample of engineered features:\")\n",
    "feature_cols = list(new_cols)\n",
    "if feature_cols:\n",
    "    print(df_clean[feature_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cfc06b9-216e-4c19-8343-6048da965a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "SECTION 1: PR√âPARATION DES DONN√âES\n",
      "==========================================================================================\n",
      "\n",
      "‚úì Dataset charg√©: 10995 cours, 29 colonnes\n",
      "‚úì Texte pr√©par√© pour 10995 cours\n",
      "\n",
      "Exemples de texte combin√©:\n",
      "   1. Tableau Server 2019.1 Administration /course/tableauserver2019/...\n",
      "   2. Fundamentals of Change Management /course/fundamentals-of-change-manag...\n",
      "   3. Microsoft Advanced Excel Dashboard : Zero To Hero (Complete) /course/a...\n",
      "\n",
      "==========================================================================================\n",
      "SECTION 2: OPTIMISATION DES PARAM√àTRES TF-IDF\n",
      "==========================================================================================\n",
      "\n",
      "Testing des combinaisons de param√®tres...\n",
      "Total de combinaisons √† tester: 162 = 162\n",
      "\n",
      "‚úì 162 combinaisons test√©es en 134.77 secondes\n",
      "\n",
      "üìä TOP 10 CONFIGURATIONS:\n",
      " max_features  max_df  min_df  ngram  sublinear_tf  n_features  sparsity    score\n",
      "          500    0.85       1 (1, 2)         False         500  0.992441 0.025993\n",
      "          500    0.85       1 (1, 2)          True         500  0.992441 0.025993\n",
      "          500    0.90       1 (1, 2)          True         500  0.992441 0.025993\n",
      "          500    0.90       1 (1, 2)         False         500  0.992441 0.025993\n",
      "          500    0.95       1 (1, 2)          True         500  0.992441 0.025993\n",
      "          500    0.95       1 (1, 2)         False         500  0.992441 0.025993\n",
      "          500    0.85       3 (1, 2)          True         500  0.992441 0.025989\n",
      "          500    0.85       3 (1, 2)         False         500  0.992441 0.025989\n",
      "          500    0.90       3 (1, 2)          True         500  0.992441 0.025989\n",
      "          500    0.90       3 (1, 2)         False         500  0.992441 0.025989\n",
      "\n",
      "üèÜ MEILLEURS PARAM√àTRES:\n",
      "   ‚Ä¢ max_features: 500\n",
      "   ‚Ä¢ max_df: 0.85\n",
      "   ‚Ä¢ min_df: 1\n",
      "   ‚Ä¢ ngram_range: (1, 2)\n",
      "   ‚Ä¢ sublinear_tf: True\n",
      "   ‚Ä¢ Score d'optimisation: 0.025993\n",
      "\n",
      "==========================================================================================\n",
      "SECTION 3: CONSTRUCTION DU MOD√àLE TF-IDF OPTIMAL\n",
      "==========================================================================================\n",
      "\n",
      "‚úì Matrice TF-IDF cr√©√©e:\n",
      "   ‚Ä¢ Dimensions: 10995 documents √ó 500 features\n",
      "   ‚Ä¢ Type: Sparse matrix (CSR)\n",
      "   ‚Ä¢ √âl√©ments non-z√©ro: 42,872\n",
      "   ‚Ä¢ Sparsit√©: 99.22%\n",
      "   ‚Ä¢ M√©moire estim√©e: 0.33 MB\n",
      "\n",
      "üìù Top 20 features (par score TF-IDF moyen):\n",
      "    1. business             ‚Üí 0.037888\n",
      "    2. trading              ‚Üí 0.028836\n",
      "    3. management           ‚Üí 0.023800\n",
      "    4. guide                ‚Üí 0.015950\n",
      "    5. financial            ‚Üí 0.015899\n",
      "    6. learn                ‚Üí 0.015629\n",
      "    7. forex                ‚Üí 0.015104\n",
      "    8. accounting           ‚Üí 0.014421\n",
      "    9. analysis             ‚Üí 0.013283\n",
      "   10. sales                ‚Üí 0.013014\n",
      "   11. start                ‚Üí 0.012855\n",
      "   12. complete             ‚Üí 0.011787\n",
      "   13. beginners            ‚Üí 0.011762\n",
      "   14. online               ‚Üí 0.011350\n",
      "   15. 2020                 ‚Üí 0.010162\n",
      "   16. investing            ‚Üí 0.010088\n",
      "   17. 1                    ‚Üí 0.009871\n",
      "   18. bitcoin              ‚Üí 0.009729\n",
      "   19. project              ‚Üí 0.009236\n",
      "   20. training             ‚Üí 0.009168\n",
      "\n",
      "==========================================================================================\n",
      "SECTION 4: SIMILARIT√â COSINUS\n",
      "==========================================================================================\n",
      "\n",
      "Calcul de la similarit√© cosinus pour 2000 cours...\n",
      "\n",
      "üìä Statistiques de similarit√© cosinus:\n",
      "   ‚Ä¢ Moyenne: 0.011763\n",
      "   ‚Ä¢ M√©diane: 0.000000\n",
      "   ‚Ä¢ √âcart-type: 0.058082\n",
      "   ‚Ä¢ Min: 0.000000\n",
      "   ‚Ä¢ Max: 1.000000\n",
      "   ‚Ä¢ Q1 (25%): 0.000000\n",
      "   ‚Ä¢ Q3 (75%): 0.000000\n",
      "   ‚Ä¢ 95e percentile: 0.079633\n",
      "   ‚Ä¢ 99e percentile: 0.310586\n",
      "\n",
      "üéØ Exemple de recommandations:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Recommandations bas√©es sur similarit√©\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ Exemple de recommandations:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m test_idx = np.random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtfidf_sample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    212\u001b[39m similarities = cosine_sim_matrix[test_idx]\n\u001b[32m    213\u001b[39m top_similar_idx = np.argsort(similarities)[::-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m6\u001b[39m]  \u001b[38;5;66;03m# Top 5 (exclure le cours lui-m√™me)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\_base.py:449\u001b[39m, in \u001b[36m_spbase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33m or shape[0]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "D√âVELOPPEMENT MOD√àLE NLP COMPLET\n",
    "================================\n",
    "- Impl√©mentation TF-IDF vectorization\n",
    "- Similarit√© cosinus et autres m√©triques\n",
    "- Optimisation des param√®tres\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: PR√âPARATION DES DONN√âES\n",
    "# ============================================================================\n",
    "print(\"=\" * 90)\n",
    "print(\"SECTION 1: PR√âPARATION DES DONN√âES\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "df = pd.read_csv('data_preprocessing.csv')\n",
    "print(f\"\\n‚úì Dataset charg√©: {df.shape[0]} cours, {df.shape[1]} colonnes\")\n",
    "\n",
    "# Pr√©paration du texte\n",
    "df['combined_text'] = df['title'].fillna('') + ' ' + df['url'].fillna('')\n",
    "df = df[df['combined_text'].str.strip() != '']\n",
    "\n",
    "print(f\"‚úì Texte pr√©par√© pour {len(df)} cours\")\n",
    "print(f\"\\nExemples de texte combin√©:\")\n",
    "for i in range(3):\n",
    "    print(f\"   {i+1}. {df['combined_text'].iloc[i][:70]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: OPTIMISATION DES PARAM√àTRES TF-IDF\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SECTION 2: OPTIMISATION DES PARAM√àTRES TF-IDF\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Grille de param√®tres √† tester\n",
    "param_grid = {\n",
    "    'max_features': [500, 1000, 2000],\n",
    "    'max_df': [0.85, 0.90, 0.95],\n",
    "    'min_df': [1, 2, 3],\n",
    "    'ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'sublinear_tf': [True, False]\n",
    "}\n",
    "\n",
    "print(\"\\nTesting des combinaisons de param√®tres...\")\n",
    "print(f\"Total de combinaisons √† tester: {3 * 3 * 3 * 3 * 2} = 162\")\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_vectorizer = None\n",
    "best_params = {}\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for max_features in param_grid['max_features']:\n",
    "    for max_df in param_grid['max_df']:\n",
    "        for min_df in param_grid['min_df']:\n",
    "            for ngram in param_grid['ngram_range']:\n",
    "                for sublinear in param_grid['sublinear_tf']:\n",
    "                    count += 1\n",
    "                    \n",
    "                    try:\n",
    "                        # Cr√©ation du vectorizer\n",
    "                        vectorizer = TfidfVectorizer(\n",
    "                            max_features=max_features,\n",
    "                            max_df=max_df,\n",
    "                            min_df=min_df,\n",
    "                            ngram_range=ngram,\n",
    "                            sublinear_tf=sublinear,\n",
    "                            stop_words='english',\n",
    "                            lowercase=True,\n",
    "                            strip_accents='unicode'\n",
    "                        )\n",
    "                        \n",
    "                        # Transformation\n",
    "                        tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "                        \n",
    "                        # Calcul des m√©triques d'√©valuation\n",
    "                        sparsity = 1 - (tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]))\n",
    "                        feature_density = tfidf_matrix.nnz / tfidf_matrix.shape[0]\n",
    "                        avg_nonzero_per_doc = tfidf_matrix.nnz / tfidf_matrix.shape[0]\n",
    "                        \n",
    "                        # Score composite (√† maximiser)\n",
    "                        score = feature_density * (1 - sparsity) / (1 + sparsity * 0.1)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'max_features': max_features,\n",
    "                            'max_df': max_df,\n",
    "                            'min_df': min_df,\n",
    "                            'ngram': ngram,\n",
    "                            'sublinear_tf': sublinear,\n",
    "                            'n_features': tfidf_matrix.shape[1],\n",
    "                            'sparsity': sparsity,\n",
    "                            'feature_density': feature_density,\n",
    "                            'avg_nonzero': avg_nonzero_per_doc,\n",
    "                            'score': score\n",
    "                        })\n",
    "                        \n",
    "                        # Mise √† jour du meilleur mod√®le\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_vectorizer = vectorizer\n",
    "                            best_params = {\n",
    "                                'max_features': max_features,\n",
    "                                'max_df': max_df,\n",
    "                                'min_df': min_df,\n",
    "                                'ngram_range': ngram,\n",
    "                                'sublinear_tf': sublinear\n",
    "                            }\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('score', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úì {count} combinaisons test√©es en {elapsed_time:.2f} secondes\")\n",
    "print(f\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "print(results_df[['max_features', 'max_df', 'min_df', 'ngram', 'sublinear_tf', \n",
    "                  'n_features', 'sparsity', 'score']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEURS PARAM√àTRES:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "print(f\"   ‚Ä¢ Score d'optimisation: {best_score:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: CONSTRUCTION DU MOD√àLE TF-IDF OPTIMAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SECTION 3: CONSTRUCTION DU MOD√àLE TF-IDF OPTIMAL\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=best_params['max_features'],\n",
    "    max_df=best_params['max_df'],\n",
    "    min_df=best_params['min_df'],\n",
    "    ngram_range=best_params['ngram_range'],\n",
    "    sublinear_tf=best_params['sublinear_tf'],\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}'\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "print(f\"\\n‚úì Matrice TF-IDF cr√©√©e:\")\n",
    "print(f\"   ‚Ä¢ Dimensions: {tfidf_matrix.shape[0]} documents √ó {tfidf_matrix.shape[1]} features\")\n",
    "print(f\"   ‚Ä¢ Type: Sparse matrix (CSR)\")\n",
    "print(f\"   ‚Ä¢ √âl√©ments non-z√©ro: {tfidf_matrix.nnz:,}\")\n",
    "print(f\"   ‚Ä¢ Sparsit√©: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.2%}\")\n",
    "print(f\"   ‚Ä¢ M√©moire estim√©e: {tfidf_matrix.data.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Features les plus importants\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "mean_tfidf = tfidf_matrix.toarray().mean(axis=0)\n",
    "top_features_idx = np.argsort(mean_tfidf)[-20:]\n",
    "\n",
    "print(f\"\\nüìù Top 20 features (par score TF-IDF moyen):\")\n",
    "top_features = feature_names[top_features_idx]\n",
    "top_scores = mean_tfidf[top_features_idx]\n",
    "for i, (feat, score) in enumerate(zip(top_features[::-1], top_scores[::-1]), 1):\n",
    "    print(f\"   {i:2d}. {feat:20s} ‚Üí {score:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: SIMILARIT√â COSINUS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SECTION 4: SIMILARIT√â COSINUS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Calcul sur √©chantillon pour efficacit√©\n",
    "sample_size = min(2000, len(df))\n",
    "sample_indices = np.random.choice(len(df), sample_size, replace=False)\n",
    "tfidf_sample = tfidf_matrix[sample_indices]\n",
    "\n",
    "print(f\"\\nCalcul de la similarit√© cosinus pour {sample_size} cours...\")\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_sample)\n",
    "\n",
    "# Statistiques de similarit√©\n",
    "upper_triangle = cosine_sim_matrix[np.triu_indices_from(cosine_sim_matrix, k=1)]\n",
    "\n",
    "print(f\"\\nüìä Statistiques de similarit√© cosinus:\")\n",
    "print(f\"   ‚Ä¢ Moyenne: {upper_triangle.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ M√©diane: {np.median(upper_triangle):.6f}\")\n",
    "print(f\"   ‚Ä¢ √âcart-type: {upper_triangle.std():.6f}\")\n",
    "print(f\"   ‚Ä¢ Min: {upper_triangle.min():.6f}\")\n",
    "print(f\"   ‚Ä¢ Max: {upper_triangle.max():.6f}\")\n",
    "print(f\"   ‚Ä¢ Q1 (25%): {np.percentile(upper_triangle, 25):.6f}\")\n",
    "print(f\"   ‚Ä¢ Q3 (75%): {np.percentile(upper_triangle, 75):.6f}\")\n",
    "print(f\"   ‚Ä¢ 95e percentile: {np.percentile(upper_triangle, 95):.6f}\")\n",
    "print(f\"   ‚Ä¢ 99e percentile: {np.percentile(upper_triangle, 99):.6f}\")\n",
    "\n",
    "# Recommandations bas√©es sur similarit√©\n",
    "print(f\"\\nüéØ Exemple de recommandations:\")\n",
    "test_idx = np.random.randint(0, len(tfidf_sample))\n",
    "similarities = cosine_sim_matrix[test_idx]\n",
    "top_similar_idx = np.argsort(similarities)[::-1][1:6]  # Top 5 (exclure le cours lui-m√™me)\n",
    "\n",
    "query_course = df.iloc[sample_indices[test_idx]]\n",
    "print(f\"\\n   Cours de requ√™te: '{query_course['title'][:60]}...'\")\n",
    "print(f\"   Rating: {query_course['rating']:.2f} | Subscribers: {query_course['num_subscribers']:,}\")\n",
    "\n",
    "print(f\"\\n   Top 5 cours similaires:\")\n",
    "for rank, idx in enumerate(top_similar_idx, 1):\n",
    "    similar_course = df.iloc[sample_indices[idx]]\n",
    "    sim_score = similarities[idx]\n",
    "    print(f\"   {rank}. {similar_course['title'][:55]}... (Similarit√©: {sim_score:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: AUTRES M√âTRIQUES DE SIMILARIT√â\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SECTION 5: AUTRES M√âTRIQUES DE SIMILARIT√â\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\nCalcul des m√©triques additionnelles...\")\n",
    "\n",
    "# Distance Euclidienne\n",
    "euclidean_dist = euclidean_distances(tfidf_sample)\n",
    "euclidean_upper = euclidean_dist[np.triu_indices_from(euclidean_dist, k=1)]\n",
    "\n",
    "# Distance Manhattan\n",
    "manhattan_dist = manhattan_distances(tfidf_sample)\n",
    "manhattan_upper = manhattan_dist[np.triu_indices_from(manhattan_dist, k=1)]\n",
    "\n",
    "# Jaccard Similarity\n",
    "def compute_jaccard(matrix):\n",
    "    \"\"\"Calcul de la similarit√© Jaccard\"\"\"\n",
    "    binary_matrix = (matrix > 0).astype(int)\n",
    "    intersection = binary_matrix.dot(binary_matrix.T).toarray()\n",
    "    union = (binary_matrix.sum(axis=1).reshape(-1, 1) + \n",
    "             binary_matrix.sum(axis=1).reshape(1, -1) - intersection)\n",
    "    jaccard = intersection / (union + 1e-10)\n",
    "    return jaccard\n",
    "\n",
    "jaccard_sim = compute_jaccard(tfidf_sample)\n",
    "jaccard_upper = jaccard_sim[np.triu_indices_from(jaccard_sim, k=1)]\n",
    "\n",
    "print(f\"\\nüìä Comparaison des m√©triques:\")\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'M√©trique': ['Cosine', 'Jaccard', 'Euclidean', 'Manhattan'],\n",
    "    'Moyenne': [\n",
    "        upper_triangle.mean(),\n",
    "        jaccard_upper.mean(),\n",
    "        euclidean_upper.mean(),\n",
    "        manhattan_upper.mean()\n",
    "    ],\n",
    "    '√âcart-type': [\n",
    "        upper_triangle.std(),\n",
    "        jaccard_upper.std(),\n",
    "        euclidean_upper.std(),\n",
    "        manhattan_upper.std()\n",
    "    ],\n",
    "    'Min': [\n",
    "        upper_triangle.min(),\n",
    "        jaccard_upper.min(),\n",
    "        euclidean_upper.min(),\n",
    "        manhattan_upper.min()\n",
    "    ],\n",
    "    'Max': [\n",
    "        upper_triangle.max(),\n",
    "        jaccard_upper.max(),\n",
    "        euclidean_upper.max(),\n",
    "        manhattan_upper.max()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(metrics_comparison.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: TOPIC MODELING (BONUS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SECTION 6: TOPIC MODELING AVEC LDA\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\nEntra√Ænement du mod√®le LDA...\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    max_df=0.9,\n",
    "    min_df=2,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=5,\n",
    "    random_state=42,\n",
    "    max_iter=15,\n",
    "    learning_method='online',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lda_model.fit(count_matrix)\n",
    "\n",
    "feature_names_lda = np.array(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(f\"\\nüìö Th√®mes d√©couverts (5 topics):\")\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_indices = topic.argsort()[-8:][::-1]\n",
    "    top_words = [feature_names_lda[i] for i in top_indices]\n",
    "    weights = topic[top_indices]\n",
    "    \n",
    "    print(f\"\\n   Topic {topic_idx + 1}:\")\n",
    "    for word, weight in zip(top_words, weights):\n",
    "        print(f\"      ‚Ä¢ {word:20s} (poids: {weight:.3f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: DIMENSIONALITY REDUCTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SECTION 7: DIMENSIONALITY REDUCTION AVEC SVD\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\nR√©duction de dimension avec Truncated SVD...\")\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"‚úì Variance expliqu√©e: {svd.explained_variance_ratio_.sum():.4f} ({svd.explained_variance_ratio_.sum()*100:.2f}%)\")\n",
    "print(f\"‚úì Forme originale: {tfidf_matrix.shape}\")\n",
    "print(f\"‚úì Forme r√©duite: {tfidf_reduced.shape}\")\n",
    "\n",
    "# Top components\n",
    "print(f\"\\nüìä Contribution des 10 premiers composants:\")\n",
    "for i, var in enumerate(svd.explained_variance_ratio_[:10], 1):\n",
    "    print(f\"   PC{i:2d}: {var*100:5.2f}% cumul: {svd.explained_variance_ratio_[:i].sum()*100:6.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: VISUALISATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SECTION 8: G√âN√âRATION DES VISUALISATIONS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Distribution de similarit√© cosinus\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(upper_triangle, bins=60, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(upper_triangle.mean(), color='red', linestyle='--', linewidth=2, label=f'Moyenne: {upper_triangle.mean():.3f}')\n",
    "ax1.axvline(np.median(upper_triangle), color='green', linestyle='--', linewidth=2, label=f'M√©diane: {np.median(upper_triangle):.3f}')\n",
    "ax1.set_xlabel('Score de similarit√©')\n",
    "ax1.set_ylabel('Fr√©quence')\n",
    "ax1.set_title('Distribution Similarit√© Cosinus')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Distribution Jaccard\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.hist(jaccard_upper, bins=60, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(jaccard_upper.mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Score Jaccard')\n",
    "ax2.set_ylabel('Fr√©quence')\n",
    "ax2.set_title('Distribution Similarit√© Jaccard')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Comparaison des m√©triques (boxplot)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "data_to_plot = [upper_triangle, jaccard_upper, \n",
    "                euclidean_upper / euclidean_upper.max(), \n",
    "                manhattan_upper / manhattan_upper.max()]\n",
    "ax3.boxplot(data_to_plot, labels=['Cosine', 'Jaccard', 'Eucl.(norm)', 'Manh.(norm)'])\n",
    "ax3.set_ylabel('Score (normalis√©)')\n",
    "ax3.set_title('Comparaison des M√©triques')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Sparsit√© vs Score\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "scatter = ax4.scatter(results_df['sparsity'], results_df['score'], \n",
    "                     c=results_df['n_features'], cmap='viridis', s=80, alpha=0.6)\n",
    "ax4.set_xlabel('Sparsit√©')\n",
    "ax4.set_ylabel('Score d\\'optimisation')\n",
    "ax4.set_title('Sparsit√© vs Score')\n",
    "plt.colorbar(scatter, ax=ax4, label='Nombre de features')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# 5. Feature Density vs Sparsity\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "scatter = ax5.scatter(results_df['feature_density'], results_df['sparsity'], \n",
    "                     c=results_df['score'], cmap='plasma', s=80, alpha=0.6)\n",
    "ax5.set_xlabel('Feature Density')\n",
    "ax5.set_ylabel('Sparsit√©')\n",
    "ax5.set_title('Feature Density vs Sparsit√©')\n",
    "plt.colorbar(scatter, ax=ax5, label='Score')\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Top Features TF-IDF\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "top_15_idx = np.argsort(mean_tfidf)[-15:]\n",
    "ax6.barh(range(len(top_15_idx)), mean_tfidf[top_15_idx], color='lightgreen')\n",
    "ax6.set_yticks(range(len(top_15_idx)))\n",
    "ax6.set_yticklabels(feature_names[top_15_idx])\n",
    "ax6.set_xlabel('Score TF-IDF moyen')\n",
    "ax6.set_title('Top 15 Features TF-IDF')\n",
    "ax6.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 7. Variance expliqu√©e SVD\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "cumsum_var = np.cumsum(svd.explained_variance_ratio_)\n",
    "ax7.plot(range(1, len(cumsum_var)+1), cumsum_var, 'b-', linewidth=2)\n",
    "ax7.axhline(0.95, color='r', linestyle='--', label='95% variance')\n",
    "ax7.axhline(0.90, color='orange', linestyle='--', label='90% variance')\n",
    "ax7.set_xlabel('Nombre de composants')\n",
    "ax7.set_ylabel('Variance cumul√©e')\n",
    "ax7.set_title('Variance Expliqu√©e - SVD')\n",
    "ax7.legend()\n",
    "ax7.grid(alpha=0.3)\n",
    "\n",
    "# 8. Ngram types distribution\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "ngram_counts = results_df['ngram'].value_counts()\n",
    "ax8.bar(range(len(ngram_counts)), ngram_counts.values, color='purple', alpha=0.7)\n",
    "ax8.set_xticks(range(len(ngram_counts)))\n",
    "ax8.set_xticklabels(ngram_counts.index)\n",
    "ax8.set_ylabel('Nombre de configurations')\n",
    "ax8.set_title('Distribution des n-grams test√©s')\n",
    "ax8.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 9. Heatmap similarit√© (petite matrice)\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "sample_sim_small = cosine_sim_matrix[:30, :30]\n",
    "im = ax9.imshow(sample_sim_small, cmap='hot', aspect='auto')\n",
    "ax9.set_title('Matrice de similarit√© (30√ó30 cours)')\n",
    "ax9.set_xlabel('Cours')\n",
    "ax9.set_ylabel('Cours')\n",
    "plt.colorbar(im, ax=ax9)\n",
    "\n",
    "plt.savefig('nlp_model_complete_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Visualisations sauvegard√©es: nlp_model_complete_analysis.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: R√âSUM√â FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"R√âSUM√â FINAL - D√âVELOPPEMENT MOD√àLE NLP\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "summary = {\n",
    "    \"üìä DONN√âES\": {\n",
    "        \"Total des cours\": len(df),\n",
    "        \"Longueur moyenne du texte\": df['combined_text'].str.len().mean()\n",
    "    },\n",
    "    \"üîß PARAM√àTRES OPTIMIS√âS\": best_params,\n",
    "    \"üìà R√âSULTATS TF-IDF\": {\n",
    "        \"Nombre de features\": tfidf_matrix.shape[1],\n",
    "        \"Sparsit√©\": f\"{(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.2%}\",\n",
    "        \"√âl√©ments non-z√©ro\": f\"{tfidf_matrix.nnz:,}\"\n",
    "    },\n",
    "    \"üìê SIMILARIT√â COSINUS\": {\n",
    "        \"Moyenne\": f\"{upper_triangle.mean():.6f}\",\n",
    "        \"√âcart-type\": f\"{upper_triangle.std():.6f}\",\n",
    "        \"Min-Max\": f\"{upper_triangle.min():.6f} - {upper_triangle.max():.6f}\"\n",
    "    },\n",
    "    \"üéØ DIMENSIONALITY REDUCTION\": {\n",
    "        \"Composants SVD\": 100,\n",
    "        \"Variance expliqu√©e\": f\"{svd.explained_variance_ratio_.sum()*100:.2f}%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for section, values in summary.items():\n",
    "    print(f\"\\n{section}\")\n",
    "    for key, value in values.items():\n",
    "        if isinstance(value, dict):\n",
    "            for k, v in value.items():\n",
    "                print(f\"   ‚Ä¢ {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"‚úÖ D√âVELOPPEMENT MOD√àLE NLP COMPLET - TERMIN√â\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa5a0-3b6d-45ba-af7c-798cf6ce2aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
